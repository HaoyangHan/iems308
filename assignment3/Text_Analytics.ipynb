{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association Rules\n",
    "## Submitted By: Anubhav Gupta\n",
    "## Date: 03/02/2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-04 00:21:42\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "print(datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "def read_document_directory(dir_path):\n",
    "    files=glob.glob(dir_path)\n",
    "    file_rows=[]\n",
    "    for file in files:\n",
    "        file_rows.extend(read_single_text_file(file))\n",
    "    return file_rows\n",
    "\n",
    "def read_single_text_file(file_path):\n",
    "    file=open(file_path, 'r', encoding=\"latin-1\")\n",
    "    rows = file.readlines()\n",
    "    for row in rows:\n",
    "        row = re.sub(r'[^\\x00-\\x7f]',r'', row) #Remove non-ascii chararacters\n",
    "    file.close()\n",
    "    return rows\n",
    "\n",
    "\n",
    "dir_path_2013 = '2013/*.txt'\n",
    "dir_path_2014 = '2014/*.txt'\n",
    "\n",
    "all_rows = read_document_directory(dir_path_2013)\n",
    "all_rows.extend(read_document_directory(dir_path_2014))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-03 16:28:28\n"
     ]
    }
   ],
   "source": [
    "#Sentence Tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "print(datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "tokenized_sentences = []\n",
    "for row in all_rows:\n",
    "    tokenized_sentences.append(sent_tokenize(row))\n",
    "\n",
    "file = open('tokenized_sentences.txt','w')\n",
    "for item in tokenized_sentences:\n",
    "    for sentence in item:\n",
    "        file.write(\"%s\\n\" %sentence.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-03 16:28:56\n"
     ]
    }
   ],
   "source": [
    "#Word Tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "f=open('tokenized_sentences.txt', 'r', encoding='utf-8')\n",
    "\n",
    "tokenized_words=[]\n",
    "for row in f:\n",
    "    row = row[2:-2]\n",
    "    tokenized_words.append(word_tokenize(row))\n",
    "f.close()\n",
    "\n",
    "file=open('tokenized_words.txt', 'w')\n",
    "for sentence in tokenized_words:\n",
    "    for word in sentence:\n",
    "        file.write(\"%s \" %word)\n",
    "    file.write(\"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-04 00:27:52\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "#Method for extracting and tokenizing dictionary of ground truth\n",
    "def get_token_dict(file_path):\n",
    "    data_dict = {}\n",
    "    f=open(file_path, 'r')\n",
    "    for row in f:\n",
    "        row = row.replace(',',' ')\n",
    "        row = row.replace('\"',' ')\n",
    "        row_vec = word_tokenize(row)\n",
    "        if row_vec[0] in data_dict.keys():\n",
    "            if row_vec not in data_dict[row_vec[0]]:\n",
    "                data_dict[row_vec[0]].append(row_vec)\n",
    "        else:\n",
    "            data_dict[row_vec[0]] = list()\n",
    "            data_dict[row_vec[0]].append(row_vec)\n",
    "    f.close()\n",
    "    return data_dict\n",
    "\n",
    "ceos_path= 'all/ceo.csv'\n",
    "companies_path = 'all/companies.csv'\n",
    "percentages_path = 'all/percentage.csv'\n",
    "not_ceos_path= 'all/notceo.csv'\n",
    "not_companies_path = 'all/notcompanies.csv'\n",
    "not_percentages_path = 'all/notpercentage.csv'\n",
    "\n",
    "ceo_dict = get_token_dict(file_path=ceos_path)\n",
    "#Remove the following from ceo_dict: White, Lord, Armstrong, Read,Smith, Mike\n",
    "stop_names = ['White', 'Lord', 'Armstrong', 'Read', 'Smith', 'Mike']\n",
    "for name in stop_names:\n",
    "    ceo_dict.pop(name)\n",
    "\n",
    "company_dict = get_token_dict(file_path=companies_path)\n",
    "percentage_dict = get_token_dict(file_path=percentages_path)\n",
    "\n",
    "not_ceo_dict = get_token_dict(file_path=not_ceos_path)\n",
    "not_company_dict = get_token_dict(file_path=not_companies_path)\n",
    "not_percentage_dict = get_token_dict(file_path=not_percentages_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-04 00:28:18\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\n",
    "#Find matching sentences from the corpus\n",
    "def find_matching_sentences(dict_tokens, tokenized_corpus):\n",
    "    results = []\n",
    "    for tokenized_sentence in tokenized_corpus:\n",
    "        for idx, token in enumerate(tokenized_sentence):\n",
    "            if token in dict_tokens:\n",
    "                for all_tokens in dict_tokens[token]:\n",
    "                    i=0\n",
    "                    flag = True\n",
    "                    for tok in all_tokens:\n",
    "                        if(idx + i >= len(tokenized_sentence) or tokenized_sentence[idx + i] != tok):\n",
    "                            flag=False\n",
    "                        i+=1\n",
    "                    if flag:\n",
    "                        matched_sent = []\n",
    "                        matched_sent.append(tokenized_sentence)\n",
    "                        matched_sent.append(all_tokens)\n",
    "                        results.append(matched_sent)\n",
    "                        break\n",
    "    return results\n",
    "\n",
    "#Positive Samples\n",
    "df_ceos = pd.DataFrame(find_matching_sentences(ceo_dict, tokenized_words), columns=['sentence','token'])\n",
    "df_comp = pd.DataFrame(find_matching_sentences(company_dict, tokenized_words), columns=['sentence','token'])\n",
    "df_percentages = pd.DataFrame(find_matching_sentences(percentage_dict, tokenized_words), columns=['sentence','token'])\n",
    "\n",
    "#Negative Samples\n",
    "df_not_ceos = pd.DataFrame(find_matching_sentences(not_ceo_dict, tokenized_words), columns=['sentence','token'])\n",
    "df_not_comp = pd.DataFrame(find_matching_sentences(not_company_dict, tokenized_words), columns=['sentence','token'])\n",
    "df_not_percentages = pd.DataFrame(find_matching_sentences(not_percentage_dict, tokenized_words), columns=['sentence','token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample negative set\n",
    "\n",
    "df_not_ceos = df_not_ceos.sample(frac=(df_ceos.shape[0]/(2*df_not_ceos.shape[0]))).reset_index().drop(['index'], axis=1)\n",
    "df_not_comp = df_not_comp.sample(frac=(df_comp.shape[0]/(2*df_not_comp.shape[0]))).reset_index().drop(['index'], axis=1)\n",
    "df_not_percentages = df_not_percentages.sample(frac=(df_percentages.shape[0]/(2*df_not_percentages.shape[0]))).reset_index().drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-04 00:38:24\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "#pos tagged sentence extraction\n",
    "df_ceos[\"pos_tagged\"] = df_ceos.apply(lambda x: nltk.pos_tag(x[\"sentence\"]), axis = 1)\n",
    "df_comp[\"pos_tagged\"] = df_comp.apply(lambda x: nltk.pos_tag(x[\"sentence\"]), axis = 1)\n",
    "df_percentages[\"pos_tagged\"] = df_percentages.apply(lambda x: nltk.pos_tag(x[\"sentence\"]), axis = 1)\n",
    "\n",
    "df_not_ceos[\"pos_tagged\"] = df_not_ceos.apply(lambda x: nltk.pos_tag(x[\"sentence\"]), axis = 1)\n",
    "df_not_comp[\"pos_tagged\"] = df_not_comp.apply(lambda x: nltk.pos_tag(x[\"sentence\"]), axis = 1)\n",
    "df_not_percentages[\"pos_tagged\"] = df_not_percentages.apply(lambda x: nltk.pos_tag(x[\"sentence\"]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-04 01:08:58\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "#Remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "set_stopwords = set(stopwords.words('English'))\n",
    "set_stopwords.remove('when')\n",
    "\n",
    "def remove_stop(pos_tagged_sentence):\n",
    "    return [word_tuple for word_tuple in pos_tagged_sentence if word_tuple[0] not in set_stopwords]\n",
    "\n",
    "df_ceos[\"stopword_removed\"] = df_ceos.apply(lambda x: remove_stop(x[\"pos_tagged\"]), axis = 1)\n",
    "df_comp[\"stopword_removed\"] = df_comp.apply(lambda x: remove_stop(x[\"pos_tagged\"]), axis = 1)\n",
    "df_percentages[\"stopword_removed\"] = df_percentages.apply(lambda x: remove_stop(x[\"pos_tagged\"]), axis = 1)\n",
    "\n",
    "df_not_ceos[\"stopword_removed\"] = df_not_ceos.apply(lambda x: remove_stop(x[\"pos_tagged\"]), axis = 1)\n",
    "df_not_comp[\"stopword_removed\"] = df_not_comp.apply(lambda x: remove_stop(x[\"pos_tagged\"]), axis = 1)\n",
    "df_not_percentages[\"stopword_removed\"] = df_not_percentages.apply(lambda x: remove_stop(x[\"pos_tagged\"]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define function for calculating word shape\n",
    "def shape(word):\n",
    "    word_shape = 'other'\n",
    "    if re.match('[0-9]+(\\.[0-9]*)?|[0-9]*\\.[0-9]+$', word):\n",
    "        word_shape = 'number'\n",
    "    elif re.match('\\W+$', word):\n",
    "        word_shape = 'punct'\n",
    "    elif re.match('[A-Z][a-z]+$', word):\n",
    "        word_shape = 'capitalized'\n",
    "    elif re.match('[A-Z]+$', word):\n",
    "        word_shape = 'uppercase'\n",
    "    elif re.match('[a-z]+$', word):\n",
    "        word_shape = 'lowercase'\n",
    "    elif re.match('[A-Z][a-z]+[A-Z][a-z]+[A-Za-z]*$', word):\n",
    "        word_shape = 'camelcase'\n",
    "    elif re.match('[A-Za-z]+$', word):\n",
    "        word_shape = 'mixedcase'\n",
    "    elif re.match('__.+__$', word):\n",
    "        word_shape = 'wildcard'\n",
    "    elif re.match('[A-Za-z0-9]+\\.$', word):\n",
    "        word_shape = 'ending-dot'\n",
    "    elif re.match('[A-Za-z0-9]+\\.[A-Za-z0-9\\.]+\\.$', word):\n",
    "        word_shape = 'abbreviation'\n",
    "    elif re.match('[A-Za-z0-9]+\\-[A-Za-z0-9\\-]+.*$', word):\n",
    "        word_shape = 'contains-hyphen'\n",
    " \n",
    "    return word_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "def create_feature_set(df_sentences, calc_neg=False, max_neg = 30000, class_true = 1, class_false = 0):\n",
    "    datapoints = []\n",
    "    neg_datapoints = []\n",
    "    for index, row in df_sentences.iterrows():\n",
    "        index_token = 0\n",
    "        for idx, token_tag in enumerate(row[\"stopword_removed\"]):\n",
    "            if token_tag[0] == row[\"token\"][0]:\n",
    "                index_token = idx\n",
    "                break\n",
    "        #Found the position of token in the string, now prepare feature vector\n",
    "        lookup_range = range(index_token, index_token+len(row[\"token\"]))\n",
    "        if calc_neg and max_neg > 0:\n",
    "            lookup_range = range(0, len(row[\"stopword_removed\"]))\n",
    "        \n",
    "        prev_word_class = class_false\n",
    "        for i in lookup_range:\n",
    "            feature_dict = {}\n",
    "            try:\n",
    "                feature_dict[\"word\"] = row[\"stopword_removed\"][i][0]\n",
    "                feature_dict[\"word_shape\"] = shape(row[\"stopword_removed\"][i][0])\n",
    "                feature_dict[\"word_len\"] = len(row[\"stopword_removed\"][i][0])\n",
    "                feature_dict[\"word_pos_tag\"] = row[\"stopword_removed\"][i][1]\n",
    "            \n",
    "                if i-1 >= 0:\n",
    "                    feature_dict[\"prev_word\"] = row[\"stopword_removed\"][i-1][0]\n",
    "                    feature_dict[\"prev_word_shape\"] = shape(row[\"stopword_removed\"][i-1][0])\n",
    "                    feature_dict[\"prev_word_len\"] = len(row[\"stopword_removed\"][i-1][0])\n",
    "                    feature_dict[\"prev_word_pos_tag\"] = row[\"stopword_removed\"][i-1][1]\n",
    "                    #feature_dict[\"prev_word_class\"] = prev_word_class\n",
    "\n",
    "                if i+1 < len(row[\"stopword_removed\"]):\n",
    "                    feature_dict[\"next_word\"] = row[\"stopword_removed\"][i+1][0]\n",
    "                    feature_dict[\"next_word_shape\"] = shape(row[\"stopword_removed\"][i+1][0])\n",
    "                    feature_dict[\"next_word_len\"] = len(row[\"stopword_removed\"][i+1][0])\n",
    "                    feature_dict[\"next_word_pos_tag\"] = row[\"stopword_removed\"][i+1][1]\n",
    "                \n",
    "            except IndexError:\n",
    "                #print(\"Error: \", row[\"stopword_removed\"], row[\"token\"], i)\n",
    "            if i in range(index_token, index_token+len(row[\"token\"])):\n",
    "                datapoints.append(feature_dict)\n",
    "                prev_word_class = class_true\n",
    "            else:\n",
    "                max_neg -= 1\n",
    "                if max_neg > 0:\n",
    "                    neg_datapoints.append(feature_dict)\n",
    "                    prev_word_class = class_false\n",
    "    return datapoints, neg_datapoints\n",
    "\n",
    "ceo_feature_set, not_ceo_feature_set= create_feature_set(df_ceos, calc_neg=True, max_neg =25000)\n",
    "comp_feature_set, not_comp_feature_set = create_feature_set(df_comp, calc_neg=True, max_neg =100000)\n",
    "percentages_feature_set, not_percentages_feature_set = create_feature_set(df_percentages, calc_neg=True, max_neg =140000)\n",
    "\n",
    "a,b = create_feature_set(df_not_ceos, calc_neg=True, max_neg =1)\n",
    "not_ceo_feature_set.extend(a)\n",
    "\n",
    "a,b = create_feature_set(df_not_comp, calc_neg=True, max_neg =1)\n",
    "not_comp_feature_set.extend(a)\n",
    "                            \n",
    "a,b=create_feature_set(df_not_percentages, calc_neg=True, max_neg =1)\n",
    "not_percentages_feature_set.extend(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-04 01:41:51\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "def get_one_hot_encoded_df(feature_set):\n",
    "    categorical_features = [\"next_word_pos_tag\", \"next_word_shape\",\"next_word_len\" , \"prev_word_pos_tag\", \n",
    "                            \"prev_word_len\", \"prev_word_shape\", \"word_shape\", \"word_pos_tag\", \"word_len\"]\n",
    "    df_features = pd.DataFrame(feature_set)\n",
    "    df_features = pd.get_dummies(df_features, columns=categorical_features)\n",
    "    return df_features\n",
    "\n",
    "df_ceo_hot = get_one_hot_encoded_df(ceo_feature_set)\n",
    "df_comp_hot = get_one_hot_encoded_df(comp_feature_set)\n",
    "df_percentages_hot = get_one_hot_encoded_df(percentages_feature_set)\n",
    "\n",
    "df_not_ceo_hot = get_one_hot_encoded_df(not_ceo_feature_set)\n",
    "df_not_comp_hot = get_one_hot_encoded_df(not_comp_feature_set)\n",
    "df_not_percentages_hot = get_one_hot_encoded_df(not_percentages_feature_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-04 01:42:12\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "def create_full_dataset(df_positive, df_negative, positive_class_label=1):\n",
    "    df1 = df_positive.copy()\n",
    "    df1['Y'] = 1\n",
    "    \n",
    "    df2 = df_negative.copy()\n",
    "    df2['Y'] = 0\n",
    "    \n",
    "    return pd.concat([df1, df2])\n",
    "\n",
    "ceo_final_dataset = create_full_dataset(df_ceo_hot, df_not_ceo_hot)\n",
    "comp_final_dataset = create_full_dataset(df_comp_hot, df_not_comp_hot)\n",
    "percentages_final_dataset = create_full_dataset(df_percentages_hot, df_not_percentages_hot)\n",
    "\n",
    "ceo_final_dataset.to_csv(\"ceo_final_dataset.csv\", index=False, header=True)\n",
    "comp_final_dataset.to_csv(\"comp_final_dataset.csv\", index=False, header=True)\n",
    "percentages_final_dataset.to_csv(\"percentages_final_dataset.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-04 01:45:40\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "#Drop irrelevant columns\n",
    "irr = [\"word\", \"next_word\", \"prev_word\"]\n",
    "ceo_final_dataset = ceo_final_dataset.drop(irr, axis=1)\n",
    "comp_final_dataset = comp_final_dataset.drop(irr, axis=1)\n",
    "percentages_final_dataset = percentages_final_dataset.drop(irr, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-04 09:19:52\n",
      "0.999863389414\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     43951\n",
      "          1       1.00      1.00      1.00     51210\n",
      "\n",
      "avg / total       1.00      1.00      1.00     95161\n",
      "\n",
      "0.998913958125\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00    118673\n",
      "          1       1.00      1.00      1.00    122570\n",
      "\n",
      "avg / total       1.00      1.00      1.00    241243\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "from sklearn import linear_model,datasets\n",
    "import pandas as pd  \n",
    "from sklearn import cross_validation, metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "\n",
    "def dataset_modeller(df, train_col_order, frac):\n",
    "    df_copy = df.copy()\n",
    "    train = df_copy.sample(frac=frac)\n",
    "    train_y = train['Y']\n",
    "    train_x = train.drop(['Y'], axis = 1).fillna(0)\n",
    "    train_x = train_x[train_col_order]\n",
    "\n",
    "    test=df_copy.drop(train.index)\n",
    "    test_y = test['Y']\n",
    "    test_x = test.drop(['Y'], axis = 1).fillna(0)\n",
    "    test_x = test_x[train_col_order]\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "def train_model(train_x, train_y):\n",
    "    classification = svm.SVC(kernel = 'rbf')\n",
    "    classification.fit(train_x,train_y)\n",
    "    return classification\n",
    "\n",
    "def test_model(classification, test_x, test_y):\n",
    "    result = classification.predict(test_x)\n",
    "    print(metrics.accuracy_score(test_y, result))\n",
    "    print(metrics.classification_report(result, test_y))\n",
    "\n",
    "def perform_all(df, train_col_order, frac=0.8):\n",
    "    train_x, train_y, test_x, test_y = dataset_modeller(df, train_col_order, frac)\n",
    "    classification =train_model(train_x, train_y)\n",
    "    test_model(classification,test_x, test_y)\n",
    "    return classification\n",
    "\n",
    "\n",
    "train_ceo_col_order = list(ceo_final_dataset)\n",
    "train_ceo_col_order.remove('Y')\n",
    "classifier_ceo = perform_all(ceo_final_dataset, train_ceo_col_order)\n",
    "\n",
    "train_comp_col_order = list(comp_final_dataset)\n",
    "train_comp_col_order.remove('Y')\n",
    "classifier_comp = perform_all(comp_final_dataset, train_comp_col_order, frac = 0.5)\n",
    "\n",
    "train_percentages_col_order = list(percentages_final_dataset)\n",
    "train_percentages_col_order.remove('Y')\n",
    "classifier_percentages = perform_all(percentages_final_dataset, train_percentages_col_order, frac = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_ceo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-04 14:19:21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['classifier_percentages.pkl']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(classifier_ceo, 'classifier_ceo.pkl') \n",
    "joblib.dump(classifier_comp, 'classifier_comp.pkl') \n",
    "joblib.dump(classifier_percentages, 'classifier_percentages.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-04 14:19:21\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "#Extract CEOs\n",
    "#Process n sentences at a time\n",
    "def create_inf_dataset(sent_tokens):\n",
    "    inference_words = []\n",
    "    for sentence in sent_tokens:\n",
    "        sentence = nltk.pos_tag(sentence)\n",
    "        sentence = remove_stop(sentence)\n",
    "        for i, token in enumerate(sentence):\n",
    "            feature_dict = {}\n",
    "            feature_dict[\"word\"] = sentence[i][0]\n",
    "            feature_dict[\"word_shape\"] = shape(sentence[i][0])\n",
    "            feature_dict[\"word_len\"] = len(sentence[i][0])\n",
    "            feature_dict[\"word_pos_tag\"] = sentence[i][1]\n",
    "\n",
    "            if i-1 >= 0:\n",
    "                feature_dict[\"prev_word\"] = sentence[i-1][0]\n",
    "                feature_dict[\"prev_word_shape\"] = shape(sentence[i-1][0])\n",
    "                feature_dict[\"prev_word_len\"] = len(sentence[i-1][0])\n",
    "                feature_dict[\"prev_word_pos_tag\"] = sentence[i-1][1]\n",
    "                feature_dict[\"prev_word_class\"] = 0 #Removing this for now\n",
    "\n",
    "            if i+1 < len(sentence):\n",
    "                feature_dict[\"next_word\"] = sentence[i+1][0]\n",
    "                feature_dict[\"next_word_shape\"] = shape(sentence[i+1][0])\n",
    "                feature_dict[\"next_word_len\"] = len(sentence[i+1][0])\n",
    "                feature_dict[\"next_word_pos_tag\"] = sentence[i+1][1]\n",
    "            inference_words.append(feature_dict)\n",
    "    return inference_words\n",
    "\n",
    "def reshape_inf(inf_feature_df, features_list):\n",
    "    cols_inf = list(inf_feature_df)\n",
    "    set_feature_list = set(features_list)\n",
    "    cols_in_features = []\n",
    "    for col in cols_inf:\n",
    "        if col in set_feature_list:\n",
    "            cols_in_features.append(col)\n",
    "    inf_feature_df = inf_feature_df[cols_in_features]\n",
    "    #Adding cols which might not be in \n",
    "    set_feature_list = set(cols_in_features)\n",
    "    for col in features_list:\n",
    "        if col not in set_feature_list:\n",
    "            inf_feature_df[col] = 0\n",
    "    return inf_feature_df[features_list].fillna(0)\n",
    "\n",
    "def do_inference(classifier, feature_x):\n",
    "    return classifier.predict(feature_x)\n",
    "\n",
    "\n",
    "def get_inferences_on_n(tokenized_words, start_index, n):\n",
    "    inf_feature_set = create_inf_dataset(tokenized_words[start_index:start_index+n])\n",
    "    inf_feature_df = get_one_hot_encoded_df(inf_feature_set)\n",
    "\n",
    "    get_col = [\"word\"]\n",
    "    word_vec = inf_feature_df[get_col]\n",
    "\n",
    "    ceo_feature_df = reshape_inf(inf_feature_df.copy(), train_ceo_col_order)\n",
    "    result_ceo = do_inference(classifier_ceo, ceo_feature_df)\n",
    "    indices_ceo = [i for i, x in enumerate(result_ceo) if x == 1]\n",
    "\n",
    "    comp_feature_df = reshape_inf(inf_feature_df.copy(), train_comp_col_order)\n",
    "    result_comp = do_inference(classifier_comp, comp_feature_df)\n",
    "    indices_comp = [i for i, x in enumerate(result_comp) if x == 1]\n",
    "\n",
    "    percentages_feature_df = reshape_inf(inf_feature_df.copy(), train_percentages_col_order)\n",
    "    result_percentages = do_inference(classifier_percentages, percentages_feature_df)\n",
    "    indices_percentages = [i for i, x in enumerate(result_percentages) if x == 1]\n",
    "\n",
    "    return word_vec.iloc[indices_ceo], word_vec.iloc[indices_comp], word_vec.iloc[indices_percentages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-04 14:19:21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anubhav\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "n = 500 #Process 500 sentences at a time\n",
    "remaining_sentences = len(tokenized_words)\n",
    "start_index = 0\n",
    "while remaining_sentences > 0:\n",
    "    if remaining_sentences < n:\n",
    "        n = remaining_sentences\n",
    "\n",
    "    words_ceo, words_comp, words_percentages = get_inferences_on_n(tokenized_words, start_index, n)\n",
    "    with open('ceo_list.csv', 'a') as f:\n",
    "        words_ceo.to_csv(f, header=False, index=False)\n",
    "    with open('companies_list.csv', 'a') as f:\n",
    "        words_comp.to_csv(f, header=False, index=False)\n",
    "    with open('percentages_list.csv', 'a') as f:\n",
    "        words_percentages.to_csv(f, header=False, index=False)\n",
    "    \n",
    "    remaining_sentences -= n\n",
    "    start_index += n\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
