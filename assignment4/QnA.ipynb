{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q & A System\n",
    "## Submitted By: Anubhav Gupta\n",
    "## Date: 03/20/2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_document_directory(dir_path):\n",
    "    files=glob.glob(dir_path)\n",
    "    file_rows=[]\n",
    "    for file in files:\n",
    "        file_rows.extend(read_single_text_file(file))\n",
    "    return file_rows\n",
    "\n",
    "def read_single_text_file(file_path):\n",
    "    file=open(file_path, 'r', encoding=\"latin-1\")\n",
    "    rows = file.readlines()\n",
    "    for row in rows:\n",
    "        row = re.sub(r'[^\\x00-\\x7f]',r'', row) #Remove non-ascii chararacters\n",
    "    file.close()\n",
    "    return rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is for a future version.\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "pos_to_wornet_dict = {\n",
    "    'JJ': wordnet.ADJ,\n",
    "    'JJR': wordnet.ADJ,\n",
    "    'JJS': wordnet.ADJ,\n",
    "    'RB': wordnet.ADV,\n",
    "    'RBR': wordnet.ADV,\n",
    "    'RBS': wordnet.ADV,\n",
    "    'NN': wordnet.NOUN,\n",
    "    'NNP': wordnet.NOUN,\n",
    "    'NNS': wordnet.NOUN,\n",
    "    'NNPS': wordnet.NOUN,\n",
    "    'VB': wordnet.VERB,\n",
    "    'VBG': wordnet.VERB,\n",
    "    'VBD': wordnet.VERB,\n",
    "    'VBN': wordnet.VERB,\n",
    "    'VBP': wordnet.VERB,\n",
    "    'VBZ': wordnet.VERB,\n",
    "}\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "stop = set(stopwords.words('english'))\n",
    "regex = r'[^A-Za-z0-9\\%\\$\\s]+'\n",
    "\n",
    "def clean_doc(doc):\n",
    "    sentences = sent_tokenize(doc)\n",
    "    \n",
    "    sent_list = []\n",
    "    for sentence in sentences:\n",
    "        #Keep only alphanumerics  \n",
    "        sentence = re.sub(regex,' ',sentence)\n",
    "    \n",
    "        #Tokenize\n",
    "        sentence_words = word_tokenize(sentence)\n",
    "    \n",
    "        #POS tag\n",
    "        sentence_pos = pos_tag(sentence_words)\n",
    "    \n",
    "        #Remove stopwords and lemmatize\n",
    "        sentence_lemma = []\n",
    "        for pos_word in sentence_pos:\n",
    "            if pos_word[0] in stop:\n",
    "                continue\n",
    "            if pos_word[1] in pos_to_wornet_dict:\n",
    "                sentence_lemma.append(lem.lemmatize(pos_word[0], pos_to_wornet_dict[pos_word[1]]).lower())\n",
    "            else:\n",
    "                sentence_lemma.append(lem.lemmatize(pos_word[0]).lower())\n",
    "        sentence_lemma.append('.')\n",
    "        sent_list.extend(sentence_lemma)\n",
    "    return ' '.join(sent_list)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "def getElasticSearchConnection():\n",
    "    es = Elasticsearch([{'host': 'localhost', 'port': 9200}])\n",
    "    return es\n",
    "\n",
    "def createIndexCorpus(rows, force=False):\n",
    "    es = getElasticSearchConnection()\n",
    "    if not es.indices.exists(index=\"docs\") or force:\n",
    "        es.indices.delete(index='docs', ignore=[400, 404])\n",
    "        #es.indices.delete(index='docs_search', ignore=[400, 404])\n",
    "        i = 0\n",
    "        for row in rows:\n",
    "            i += 1\n",
    "            es.index(index='docs', doc_type='docs_type', id=i, body={\"text\":row})\n",
    "            #es.index(index='docs_search', doc_type='docs_clean_type', id=i, body={\"text\":clean_doc(row)})\n",
    "            print(i)\n",
    "    return es\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#One time Processing\n",
    "\n",
    "#Document directories\n",
    "dir_path_2013 = '2013/*.txt'\n",
    "dir_path_2014 = '2014/*.txt'\n",
    "#Extract all rows from all the txt_docs. 1 row == 1 doc\n",
    "all_rows = read_document_directory(dir_path_2013)\n",
    "all_rows.extend(read_document_directory(dir_path_2014))\n",
    "#Perform data cleaning on the rows\n",
    "\n",
    "#Load the documents into elastic search. If the index already exists, don't create again\n",
    "es = createIndexCorpus(all_rows, force=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags, ne_chunk\n",
    "\n",
    "pos_to_wornet_dict = {\n",
    "    'JJ': wordnet.ADJ,\n",
    "    'JJR': wordnet.ADJ,\n",
    "    'JJS': wordnet.ADJ,\n",
    "    'RB': wordnet.ADV,\n",
    "    'RBR': wordnet.ADV,\n",
    "    'RBS': wordnet.ADV,\n",
    "    'NN': wordnet.NOUN,\n",
    "    'NNP': wordnet.NOUN,\n",
    "    'NNS': wordnet.NOUN,\n",
    "    'NNPS': wordnet.NOUN,\n",
    "    'VB': wordnet.VERB,\n",
    "    'VBG': wordnet.VERB,\n",
    "    'VBD': wordnet.VERB,\n",
    "    'VBN': wordnet.VERB,\n",
    "    'VBP': wordnet.VERB,\n",
    "    'VBZ': wordnet.VERB,\n",
    "}\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "def clean_sentence(sentence, keep_stop = {}):\n",
    "    #Keep only alphanumerics\n",
    "    regex = r'[^A-Za-z0-9\\%\\$\\s]+'\n",
    "    sentence = re.sub(regex,' ',sentence)\n",
    "    \n",
    "    #Tokenize\n",
    "    sentence_words = word_tokenize(sentence)\n",
    "    \n",
    "    #POS tag\n",
    "    sentence_pos = pos_tag(sentence_words)\n",
    "    \n",
    "    #NER tag\n",
    "    ne_tree = ne_chunk(sentence_pos)\n",
    "    iob_tagged = tree2conlltags(ne_tree)\n",
    "    \n",
    "    #Remove stopwords and lemmatize\n",
    "    sentence_lemma = []\n",
    "    for pos_word in iob_tagged:\n",
    "        if pos_word[0] in stop:\n",
    "            if pos_word[0] not in keep_stop:\n",
    "                continue\n",
    "        if pos_word[1] in pos_to_wornet_dict:\n",
    "            sentence_lemma.append((lem.lemmatize(pos_word[0], pos_to_wornet_dict[pos_word[1]]), pos_word[1], pos_word[2], pos_word[0]))\n",
    "        else:\n",
    "            sentence_lemma.append((lem.lemmatize(pos_word[0]), pos_word[1], pos_word[2], pos_word[0]))    \n",
    "    \n",
    "    #Bigram Iterator\n",
    "    it = nltk.bigrams(sentence_lemma)\n",
    "    \n",
    "    return sentence, sentence_lemma, it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############# Document Selection#######################3\n",
    "wh_words = ['what', 'where', 'who', 'how', 'when', 'whose', 'whom', 'why', 'which']\n",
    "\n",
    "def extract_question_ner_tags(question):\n",
    "    sentence, sentence_lemma, it = clean_sentence(question, keep_stop = set(wh_words))\n",
    "    lower_question = [tag[0].lower() for tag in sentence_lemma]\n",
    "\n",
    "    id_word = 0\n",
    "    q_word = 'unknown'\n",
    "    for idx, word in enumerate(lower_question):\n",
    "        if word in wh_words:\n",
    "            id_word = idx\n",
    "            q_word = word\n",
    "            break    \n",
    "    question_ner_tags = []\n",
    "    if q_word == 'who' or q_word == 'whom' or q_word=='whose':\n",
    "        question_ner_tags = ['B-PERSON', 'I-PERSON']\n",
    "    elif q_word == 'which' or q_word == 'what':\n",
    "        next_word = sentence_lemma[id_word+1][0]\n",
    "        if  next_word in ['company', 'organization', 'firm']:\n",
    "            question_ner_tags = ['B-ORGANIZATION', 'I-ORGANIZATION']\n",
    "        elif next_word in ['person', 'people', 'name', 'man', 'woman']:\n",
    "            question_ner_tags = ['B-PERSON', 'I-PERSON']\n",
    "        elif next_word in ['percentage', 'quantity', 'amount', 'price', 'weight', 'measure']:\n",
    "            question_ner_tags = ['O']\n",
    "        else:\n",
    "            question_ner_tags = ['B-NP', \"I-NP\", 'O']\n",
    "    elif q_word == 'where':\n",
    "        question_ner_tags = ['B-GPE', 'I-GPE']\n",
    "    return question_ner_tags\n",
    "        \n",
    "        \n",
    "def extract_keywords(question):\n",
    "    q, q_lemma, bi_it = clean_sentence(question)\n",
    "    lower_question = [tag[0].lower() for tag in q_lemma]\n",
    "    \n",
    "    q_lemma_question_removed = []\n",
    "    for idx, word in enumerate(lower_question):\n",
    "        if word not in wh_words:\n",
    "            q_lemma_question_removed.append(q_lemma[idx]) \n",
    "            \n",
    "    keywords = {}\n",
    "    for word in q_lemma_question_removed:\n",
    "        word_orig = word[0]\n",
    "        word_lemma = word[3]\n",
    "        if word_orig not in keywords:\n",
    "            keywords[word_orig] = 1\n",
    "        else:\n",
    "            keywords[word_orig] = keywords[word_orig] + 1\n",
    "        if word_lemma != word_orig:\n",
    "            if word_lemma not in keywords:\n",
    "                keywords[word_lemma] = 1\n",
    "            else:\n",
    "                keywords[word_lemma] = keywords[word_lemma] + 1\n",
    "    return keywords\n",
    "\n",
    "def findTopNDocs(keywords, n = 1):\n",
    "    stri = \"\"\n",
    "    for key in keywords.keys():\n",
    "        stri += key + \" \"\n",
    "    es = getElasticSearchConnection()\n",
    "    return es.search(index=\"docs\", size=n, body={\"query\": {\"match\": {'text': stri}}})\n",
    "    \n",
    "def flatten_es_to_list(es_type):\n",
    "    list_docs = []\n",
    "    for doc in es_type[\"hits\"][\"hits\"]:\n",
    "        list_docs.append(doc[\"_source\"][\"text\"])\n",
    "    return list_docs\n",
    "\n",
    "def findRelevantDocs(question, num):\n",
    "    keywords = extract_keywords(question)\n",
    "    print(keywords)\n",
    "    top_docs = findTopNDocs(keywords, num)\n",
    "    return keywords, top_docs, flatten_es_to_list(top_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################Answer Selection########################\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag\n",
    "import math\n",
    "from operator import itemgetter\n",
    "\n",
    "def flatten_docs_to_sentences(flatten_docs):\n",
    "    sentences = []\n",
    "    for doc in flatten_docs:\n",
    "        sentences.extend(sent_tokenize(doc))\n",
    "    return sentences\n",
    "\n",
    "def score_sentences(sentences, keywords, target_ner_tags):\n",
    "    results = []\n",
    "    sent_index = 0\n",
    "    for idx, sent in enumerate(sentences):\n",
    "        sent_text, sent_lemma, bi_iterator = clean_sentence(sent)\n",
    "        #Don't consider sentences which don't have target NER tags\n",
    "        continue_process = False\n",
    "        for word_lemma in sent_lemma:\n",
    "            if word_lemma[2] in target_ner_tags:\n",
    "                continue_process = True\n",
    "                break\n",
    "        if not continue_process:\n",
    "            continue\n",
    "        #The number of words in the candidate sentence that occur adjacently in both the question\n",
    "        #and the answer candidate (+)\n",
    "        score1 = 0\n",
    "        for bigr in bi_iterator:\n",
    "            if bigr[0][0] in keywords and bigr[1][0] in keywords:\n",
    "                score1 += 1\n",
    "        \n",
    "        #- The TF-IDF sum of the number of words that matched between question and answer (+)\n",
    "        #- The TF-IDF sum of the number of question content words that did not match in the answer (-)\n",
    "        freq_word = dict()\n",
    "        for word_lemma in sent_lemma:\n",
    "            word = word_lemma[0]\n",
    "            if word in freq_word:\n",
    "                freq_word[word] = freq_word[word]+1\n",
    "            else:\n",
    "                freq_word[word] = 1\n",
    "        \n",
    "        matched_tf_idf = 0\n",
    "        unmatched_tf_idf = 0\n",
    "        for word_lemma in sent_lemma:\n",
    "            word = word_lemma[0]\n",
    "            tf_word = freq_word[word]\n",
    "            idf_word = math.log(float(len(sentences) - tf_word + 0.5)/float(tf_word + 0.5))\n",
    "            tf_idf_word = tf_word*idf_word\n",
    "            if word in keywords:\n",
    "                matched_tf_idf += tf_idf_word\n",
    "            else:\n",
    "                unmatched_tf_idf += tf_idf_word\n",
    "        if matched_tf_idf > 0:\n",
    "            results.append((sent, sent_lemma, score1, matched_tf_idf, unmatched_tf_idf, matched_tf_idf-unmatched_tf_idf))\n",
    "    results.sort(key=itemgetter(2, 3), reverse=True)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask me anything: Who is the CEO of Apple?\n",
      "{'CEO': 1, 'Apple': 1}\n",
      "I think the answer might be in:  What happened yesterday, in case you missed it, is that a famous investor named Carl Icahn announced in a Tweet that he had bought a bunch of Apple stock and had just talked to Apple CEO Tim Cook on the phone.\n",
      "Ask me anything: Which company is the world leader in search technology?\n",
      "{'company': 1, 'world': 1, 'leader': 1, 'search': 1, 'technology': 1}\n",
      "I think the answer might be in:  With Microsoft technology powering searches and search advertising for both companies, the partners hoped to mount a more competitive challenge to Google, the world's No.\n",
      "Ask me anything: Which company went bankrupt in month September of year 2008?\n",
      "{'company': 1, 'go': 1, 'went': 1, 'bankrupt': 1, 'month': 1, 'September': 1, 'year': 1, '2008': 1}\n",
      "I think the answer might be in:  Most will remember September 2008, which was when the credit crisis got really ugly as Lehman Brothers went bankrupt and interest rates surged.\n",
      "Ask me anything: Who is the CEO of Google?\n",
      "{'CEO': 1, 'Google': 1}\n",
      "I think the answer might be in:  Larry Page (CEO of Google) Net Worth: $32.3 billion 4.\n",
      "Ask me anything: What is greenhouse effect?\n",
      "{'greenhouse': 1, 'effect': 1}\n",
      "I think the answer might be in:  House Speaker John Boehner (R-Ohio) isn't happy with US President Barack Obama's new agreement with China to lower greenhouse gas emissions.\n"
     ]
    }
   ],
   "source": [
    "#Run everything above this one time. Then this cell could be run multiple time or in a server.\n",
    "def takeQuestion():\n",
    "    question = input(\"Ask me anything: \")\n",
    "    return question\n",
    "\n",
    "q = 5\n",
    "while(q > 0):\n",
    "    question = takeQuestion()\n",
    "    keywords, docs, flatten_docs = findRelevantDocs(question, 5)\n",
    "    sents = flatten_docs_to_sentences(flatten_docs)\n",
    "    target_ner_tags = extract_question_ner_tags(question)\n",
    "    score_sents = score_sentences(sents, keywords, target_ner_tags)\n",
    "    if len(score_sents) > 0:\n",
    "        print(\"I think the answer might be in: \", score_sents[0][0])\n",
    "    else:\n",
    "        print(\"I don't know that one!\")\n",
    "    q -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from nltk.tag import StanfordNERTagger\n",
    "#from nltk.tokenize import word_tokenize\n",
    "\n",
    "#st = StanfordNERTagger('C:\\\\Users\\\\Anubhav\\\\Desktop\\\\git\\\\iems308\\\\assignment4\\\\stanford\\\\stanford-ner-2018-02-27\\\\classifiers\\\\english.muc.7class.distsim.crf.ser',\n",
    "#                       'C:/Users/Anubhav/Desktop/git/iems308/assignment4/stanford/stanford-ner-2018-02-27/stanford-ner.jar',\n",
    "#                       encoding='utf-8')\n",
    "\n",
    "#text = 'While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.'\n",
    "\n",
    "#tokenized_text = word_tokenize(text)\n",
    "#classified_text = st.tag(tokenized_text)\n",
    "\n",
    "#print(classified_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
