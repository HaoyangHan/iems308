{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question and Answer System\n",
    "### Submitted By: Anubhav Gupta\n",
    "### Date: 03/23/2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading documents from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_document_directory(dir_path):\n",
    "    files=glob.glob(dir_path)\n",
    "    file_rows=[]\n",
    "    for file in files:\n",
    "        file_rows.extend(read_single_text_file(file))\n",
    "    return file_rows\n",
    "\n",
    "def read_single_text_file(file_path):\n",
    "    file=open(file_path, 'r', encoding=\"latin-1\")\n",
    "    rows = file.readlines()\n",
    "    for row in rows:\n",
    "        row = re.sub(r'[^\\x00-\\x7f]',r'', row) #Remove non-ascii chararacters\n",
    "    file.close()\n",
    "    return rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is a method for future version of this system which will clean the documents before pushing to Elastic Search. It will ensure better search and relevancy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is for a future version.\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "pos_to_wornet_dict = {\n",
    "    'JJ': wordnet.ADJ,\n",
    "    'JJR': wordnet.ADJ,\n",
    "    'JJS': wordnet.ADJ,\n",
    "    'RB': wordnet.ADV,\n",
    "    'RBR': wordnet.ADV,\n",
    "    'RBS': wordnet.ADV,\n",
    "    'NN': wordnet.NOUN,\n",
    "    'NNP': wordnet.NOUN,\n",
    "    'NNS': wordnet.NOUN,\n",
    "    'NNPS': wordnet.NOUN,\n",
    "    'VB': wordnet.VERB,\n",
    "    'VBG': wordnet.VERB,\n",
    "    'VBD': wordnet.VERB,\n",
    "    'VBN': wordnet.VERB,\n",
    "    'VBP': wordnet.VERB,\n",
    "    'VBZ': wordnet.VERB,\n",
    "}\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "stop = set(stopwords.words('english'))\n",
    "regex = r'[^A-Za-z0-9\\%\\$\\s]+'\n",
    "\n",
    "def clean_doc(doc):\n",
    "    sentences = sent_tokenize(doc)\n",
    "    \n",
    "    sent_list = []\n",
    "    for sentence in sentences:\n",
    "        #Keep only alphanumerics  \n",
    "        sentence = re.sub(regex,' ',sentence)\n",
    "    \n",
    "        #Tokenize\n",
    "        sentence_words = word_tokenize(sentence)\n",
    "    \n",
    "        #POS tag\n",
    "        sentence_pos = pos_tag(sentence_words)\n",
    "    \n",
    "        #Remove stopwords and lemmatize\n",
    "        sentence_lemma = []\n",
    "        for pos_word in sentence_pos:\n",
    "            if pos_word[0] in stop:\n",
    "                continue\n",
    "            if pos_word[1] in pos_to_wornet_dict:\n",
    "                sentence_lemma.append(lem.lemmatize(pos_word[0], pos_to_wornet_dict[pos_word[1]]).lower())\n",
    "            else:\n",
    "                sentence_lemma.append(lem.lemmatize(pos_word[0]).lower())\n",
    "        sentence_lemma.append('.')\n",
    "        sent_list.extend(sentence_lemma)\n",
    "    return ' '.join(sent_list)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting documents into ElasticSearch for indexing. ElasticSearch provides fast search on the text and also provides relevancy score based on Okapi formula when matching with a query of keywords. Thus it handles the retrieval of top N documents from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "def getElasticSearchConnection():\n",
    "    es = Elasticsearch([{'host': 'localhost', 'port': 9200}])\n",
    "    return es\n",
    "\n",
    "def createIndexCorpus(rows, force=False):\n",
    "    es = getElasticSearchConnection()\n",
    "    if not es.indices.exists(index=\"docs\") or force:\n",
    "        es.indices.delete(index='docs', ignore=[400, 404])\n",
    "        #es.indices.delete(index='docs_search', ignore=[400, 404])\n",
    "        i = 0\n",
    "        for row in rows:\n",
    "            i += 1\n",
    "            es.index(index='docs', doc_type='docs_type', id=i, body={\"text\":row})\n",
    "            #es.index(index='docs_search', doc_type='docs_clean_type', id=i, body={\"text\":clean_doc(row)})\n",
    "            print(i)\n",
    "    return es\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, the code below needs to be executed only once. This segment populates the ElasticSearch index once in the lifetime of the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#One time Processing\n",
    "\n",
    "#Document directories\n",
    "dir_path_2013 = '2013/*.txt'\n",
    "dir_path_2014 = '2014/*.txt'\n",
    "#Extract all rows from all the txt_docs. 1 row == 1 doc\n",
    "all_rows = read_document_directory(dir_path_2013)\n",
    "all_rows.extend(read_document_directory(dir_path_2014))\n",
    "#Perform data cleaning on the rows\n",
    "\n",
    "#Load the documents into elastic search. If the index already exists, don't create again\n",
    "es = createIndexCorpus(all_rows, force=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following code is generic cleaning function for all the sentences (including questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags, ne_chunk\n",
    "\n",
    "pos_to_wornet_dict = {\n",
    "    'JJ': wordnet.ADJ,\n",
    "    'JJR': wordnet.ADJ,\n",
    "    'JJS': wordnet.ADJ,\n",
    "    'RB': wordnet.ADV,\n",
    "    'RBR': wordnet.ADV,\n",
    "    'RBS': wordnet.ADV,\n",
    "    'NN': wordnet.NOUN,\n",
    "    'NNP': wordnet.NOUN,\n",
    "    'NNS': wordnet.NOUN,\n",
    "    'NNPS': wordnet.NOUN,\n",
    "    'VB': wordnet.VERB,\n",
    "    'VBG': wordnet.VERB,\n",
    "    'VBD': wordnet.VERB,\n",
    "    'VBN': wordnet.VERB,\n",
    "    'VBP': wordnet.VERB,\n",
    "    'VBZ': wordnet.VERB,\n",
    "}\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "def clean_sentence(sentence, keep_stop = {}):\n",
    "    #Keep only alphanumerics\n",
    "    regex = r'[^A-Za-z0-9\\%\\$\\s]+'\n",
    "    sentence = re.sub(regex,' ',sentence)\n",
    "    \n",
    "    #Tokenize\n",
    "    sentence_words = word_tokenize(sentence)\n",
    "    \n",
    "    #POS tag\n",
    "    sentence_pos = pos_tag(sentence_words)\n",
    "    \n",
    "    #NER tag\n",
    "    ne_tree = ne_chunk(sentence_pos)\n",
    "    iob_tagged = tree2conlltags(ne_tree)\n",
    "    \n",
    "    #Remove stopwords and lemmatize\n",
    "    sentence_lemma = []\n",
    "    for pos_word in iob_tagged:\n",
    "        if pos_word[0] in stop:\n",
    "            if pos_word[0] not in keep_stop:\n",
    "                continue\n",
    "        if pos_word[1] in pos_to_wornet_dict:\n",
    "            sentence_lemma.append((lem.lemmatize(pos_word[0], pos_to_wornet_dict[pos_word[1]]), pos_word[1], pos_word[2], pos_word[0]))\n",
    "        else:\n",
    "            sentence_lemma.append((lem.lemmatize(pos_word[0]), pos_word[1], pos_word[2], pos_word[0]))    \n",
    "    \n",
    "    #Bigram Iterator\n",
    "    it = nltk.bigrams(sentence_lemma)\n",
    "    \n",
    "    return sentence, sentence_lemma, it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This section deals with the following:\n",
    "1. Determining question type and finding out nltk NER chunker compatible tags\n",
    "2. Keyword Extraction\n",
    "3. Document selection and scoring using ElasticSearch to fetch top N documents relevant to the Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############# Document Selection#######################3\n",
    "wh_words = ['what', 'where', 'who', 'how', 'when', 'whose', 'whom', 'why', 'which']\n",
    "\n",
    "def extract_question_ner_tags(question):\n",
    "    sentence, sentence_lemma, it = clean_sentence(question, keep_stop = set(wh_words))\n",
    "    lower_question = [tag[0].lower() for tag in sentence_lemma]\n",
    "\n",
    "    id_word = 0\n",
    "    q_word = 'unknown'\n",
    "    for idx, word in enumerate(lower_question):\n",
    "        if word in wh_words:\n",
    "            id_word = idx\n",
    "            q_word = word\n",
    "            break    \n",
    "    question_ner_tags = []\n",
    "    if q_word == 'who' or q_word == 'whom' or q_word=='whose':\n",
    "        question_ner_tags = ['B-PERSON', 'I-PERSON']\n",
    "    elif q_word == 'which' or q_word == 'what':\n",
    "        next_word = sentence_lemma[id_word+1][0]\n",
    "        if  next_word in ['company', 'organization', 'firm']:\n",
    "            question_ner_tags = ['B-ORGANIZATION', 'I-ORGANIZATION']\n",
    "        elif next_word in ['person', 'people', 'name', 'man', 'woman']:\n",
    "            question_ner_tags = ['B-PERSON', 'I-PERSON']\n",
    "        elif next_word in ['percentage', 'quantity', 'amount', 'price', 'weight', 'measure']:\n",
    "            question_ner_tags = ['O']\n",
    "        else:\n",
    "            question_ner_tags = ['B-NP', \"I-NP\", 'O']\n",
    "    elif q_word == 'where':\n",
    "        question_ner_tags = ['B-GPE', 'I-GPE']\n",
    "    return question_ner_tags\n",
    "        \n",
    "        \n",
    "def extract_keywords(question):\n",
    "    q, q_lemma, bi_it = clean_sentence(question)\n",
    "    lower_question = [tag[0].lower() for tag in q_lemma]\n",
    "    \n",
    "    q_lemma_question_removed = []\n",
    "    for idx, word in enumerate(lower_question):\n",
    "        if word not in wh_words:\n",
    "            q_lemma_question_removed.append(q_lemma[idx]) \n",
    "            \n",
    "    keywords = {}\n",
    "    for word in q_lemma_question_removed:\n",
    "        word_orig = word[0]\n",
    "        word_lemma = word[3]\n",
    "        if word_orig not in keywords:\n",
    "            keywords[word_orig] = 1\n",
    "        else:\n",
    "            keywords[word_orig] = keywords[word_orig] + 1\n",
    "        if word_lemma != word_orig:\n",
    "            if word_lemma not in keywords:\n",
    "                keywords[word_lemma] = 1\n",
    "            else:\n",
    "                keywords[word_lemma] = keywords[word_lemma] + 1\n",
    "    return keywords\n",
    "\n",
    "def findTopNDocs(keywords, n = 1):\n",
    "    stri = \"\"\n",
    "    for key in keywords.keys():\n",
    "        stri += key + \" \"\n",
    "    es = getElasticSearchConnection()\n",
    "    return es.search(index=\"docs\", size=n, body={\"query\": {\"match\": {'text': stri}}})\n",
    "    \n",
    "def flatten_es_to_list(es_type):\n",
    "    list_docs = []\n",
    "    for doc in es_type[\"hits\"][\"hits\"]:\n",
    "        list_docs.append(doc[\"_source\"][\"text\"])\n",
    "    return list_docs\n",
    "\n",
    "def findRelevantDocs(question, num):\n",
    "    keywords = extract_keywords(question)\n",
    "    print(keywords)\n",
    "    top_docs = findTopNDocs(keywords, num)\n",
    "    return keywords, top_docs, flatten_es_to_list(top_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This section deals with the following:\n",
    "1. Sentence Extraction from the documents\n",
    "2. Selecting only the sentences which have matching NER tags determined from the question type\n",
    "3. Scoring the sentences based on the following criteria:\n",
    "    * The number of words in the candidate sentence that occur adjacently in both the question and the answer candidate (+)\n",
    "    * The TF-IDF sum of the number of words that matched between question and answer (+)\n",
    "    * The TF-IDF sum of the number of question content words that did not match in the answer (-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################Answer Selection########################\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag\n",
    "import math\n",
    "from operator import itemgetter\n",
    "\n",
    "def flatten_docs_to_sentences(flatten_docs):\n",
    "    sentences = []\n",
    "    for doc in flatten_docs:\n",
    "        sentences.extend(sent_tokenize(doc))\n",
    "    return sentences\n",
    "\n",
    "def score_sentences(sentences, keywords, target_ner_tags):\n",
    "    results = []\n",
    "    sent_index = 0\n",
    "    for idx, sent in enumerate(sentences):\n",
    "        sent_text, sent_lemma, bi_iterator = clean_sentence(sent)\n",
    "        #Don't consider sentences which don't have target NER tags\n",
    "        continue_process = False\n",
    "        for word_lemma in sent_lemma:\n",
    "            if word_lemma[2] in target_ner_tags:\n",
    "                continue_process = True\n",
    "                break\n",
    "        if not continue_process:\n",
    "            continue\n",
    "        #The number of words in the candidate sentence that occur adjacently in both the question\n",
    "        #and the answer candidate (+)\n",
    "        score1 = 0\n",
    "        for bigr in bi_iterator:\n",
    "            if bigr[0][0] in keywords and bigr[1][0] in keywords:\n",
    "                score1 += 1\n",
    "        \n",
    "        #- The TF-IDF sum of the number of words that matched between question and answer (+)\n",
    "        #- The TF-IDF sum of the number of question content words that did not match in the answer (-)\n",
    "        freq_word = dict()\n",
    "        for word_lemma in sent_lemma:\n",
    "            word = word_lemma[0]\n",
    "            if word in freq_word:\n",
    "                freq_word[word] = freq_word[word]+1\n",
    "            else:\n",
    "                freq_word[word] = 1\n",
    "        \n",
    "        matched_tf_idf = 0\n",
    "        unmatched_tf_idf = 0\n",
    "        for word_lemma in sent_lemma:\n",
    "            word = word_lemma[0]\n",
    "            tf_word = freq_word[word]\n",
    "            idf_word = math.log(float(len(sentences) - tf_word + 0.5)/float(tf_word + 0.5))\n",
    "            tf_idf_word = tf_word*idf_word\n",
    "            if word in keywords:\n",
    "                matched_tf_idf += tf_idf_word\n",
    "            else:\n",
    "                unmatched_tf_idf += tf_idf_word\n",
    "        if matched_tf_idf > 0:\n",
    "            results.append((sent, sent_lemma, score1, matched_tf_idf, unmatched_tf_idf, matched_tf_idf-unmatched_tf_idf))\n",
    "    results.sort(key=itemgetter(2, 3), reverse=True)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This method encapsulates the entire Q&A process using the functions defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_answer_sentence(question):\n",
    "    keywords, docs, flatten_docs = findRelevantDocs(question, 5)\n",
    "    sents = flatten_docs_to_sentences(flatten_docs)\n",
    "    target_ner_tags = extract_question_ner_tags(question)\n",
    "    score_sents = score_sentences(sents, keywords, target_ner_tags)\n",
    "    if len(score_sents) > 0:\n",
    "        return (\"I think the answer might be in: \" + score_sents[0][0])\n",
    "    else:\n",
    "        return (\"I don't know that one!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Cases: This section runs the entire system for specific question types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POC For CEO Question Type:\n",
      "Question:  Who is the CEO of Google?\n",
      "{'CEO': 1, 'Google': 1}\n",
      "I think the answer might be in: Larry Page (CEO of Google) Net Worth: $32.3 billion 4.\n",
      "Question:  Who is the CEO of Facebook?\n",
      "{'CEO': 1, 'Facebook': 1}\n",
      "I think the answer might be in: Mark Zuckerberg, CEO of Facebook, when Facebook's IPO flopped, the talk was that the young Zuckerberg wasn't ready to be a strong leader of a major company.\n",
      "Question:  Who is the CEO of Twitter?\n",
      "{'CEO': 1, 'Twitter': 1}\n",
      "I think the answer might be in: In addition to being Twitter's CEO, Evan Williams was its largest shareholder.\n",
      "Question:  Who is the CEO of Amazon?\n",
      "{'CEO': 1, 'Amazon': 1}\n",
      "I think the answer might be in: Prime members can also borrow more than 700,000 books, listen to one million songs and hundreds of playlists, save unlimited photos and watch tens of thousands of movies and TV episodes including the Golden Globe nominated show from Amazon Studios, Transparent, said Jeff Bezos, founder and CEO of Amazon.com.\n",
      "\n",
      "\n",
      "POC For company bankrupt Question Type:\n",
      "Question:  Which companies went bankrupt in month September of year 2008?\n",
      "{'company': 1, 'companies': 1, 'go': 1, 'went': 1, 'bankrupt': 1, 'month': 1, 'September': 1, 'year': 1, '2008': 1}\n",
      "I think the answer might be in: Most will remember September 2008, which was when the credit crisis got really ugly as Lehman Brothers went bankrupt and interest rates surged.\n",
      "Question:  Which companies went bankrupt in month may of year 2009?\n",
      "{'company': 1, 'companies': 1, 'go': 1, 'went': 1, 'bankrupt': 1, 'month': 1, 'may': 1, 'year': 1, '2009': 1}\n",
      "I think the answer might be in: In 2009, it went bankrupt, shuttered all but one of its 33 plants in the US, let go 25,000 workers, and shifted its center of gravity to Shanghai.\n",
      "Question:  Which companies went bankrupt in month december of year 2001?\n",
      "{'company': 1, 'companies': 1, 'go': 1, 'went': 1, 'bankrupt': 1, 'month': 1, 'december': 1, 'year': 1, '2001': 1}\n",
      "I think the answer might be in: In the early 2000s, the industry was in turmoil, with a glut of fiber being built across the U.S. Before its collapse amid an accounting scandal, Enron Corp dabbled in fiber routes, while WorldCom was losing money on its fiber networks before its top executives committed fraud and the company went bankrupt.\n",
      "\n",
      "\n",
      "POC For description type questions:\n",
      "Question:  What affects GDP?\n",
      "{'affect': 1, 'affects': 1, 'GDP': 1}\n",
      "I think the answer might be in: \"The revision of 2013 GDP could affect the size of 2014 GDP but will basically not affect GDP growth for 2014,\" the bureau said in a statement.\n",
      "Question:  What affects interest rates?\n",
      "{'affect': 1, 'affects': 1, 'interest': 1, 'rate': 1, 'rates': 1}\n",
      "I think the answer might be in: LSAPs, in contrast, most directly affect term premiums...Â As both forward rate guidance and LSAPs affect longer-term interest rates, the use of these tools allows monetary policy to be effective even when short-term interest rates are close to zero The FOMC also has attempted to credibly communicate to the market \"the criteria that would inform future decisions about the program.\"\n",
      "Question:  What affects inflation index?\n",
      "{'affect': 1, 'affects': 1, 'inflation': 1, 'index': 1}\n",
      "I think the answer might be in: How Does the Value of the Dollar Influence Inflation?One important way the dollarâs value affects inflation is through commodity prices.\n"
     ]
    }
   ],
   "source": [
    "def print_question_answer(question):\n",
    "    print(\"Question: \", question)\n",
    "    print(get_answer_sentence(question))\n",
    "\n",
    "#CEO test cases\n",
    "print(\"POC For CEO Question Type:\")\n",
    "print_question_answer(\"Who is the CEO of Google?\")\n",
    "print_question_answer(\"Who is the CEO of Facebook?\")\n",
    "print_question_answer(\"Who is the CEO of Twitter?\")\n",
    "print_question_answer(\"Who is the CEO of Amazon?\")\n",
    "\n",
    "print(\"\\n\\nPOC For company bankrupt Question Type:\")\n",
    "print_question_answer(\"Which companies went bankrupt in month September of year 2008?\")\n",
    "print_question_answer(\"Which companies went bankrupt in month may of year 2009?\")\n",
    "print_question_answer(\"Which companies went bankrupt in month december of year 2001?\")\n",
    "\n",
    "print(\"\\n\\nPOC For description type questions:\")\n",
    "print_question_answer(\"What affects GDP?\")\n",
    "print_question_answer(\"What affects interest rates?\")\n",
    "print_question_answer(\"What affects inflation index?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This method can be run in a server environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can run the following code in a server environment.\n",
    "def takeQuestion():\n",
    "    question = input(\"Ask me anything: \")\n",
    "    return question\n",
    "\n",
    "q = 5\n",
    "while(q > 0):\n",
    "    question = takeQuestion()\n",
    "    print(get_answer_sentence(question))\n",
    "    q -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Again, for a future version, we could use Stanford's NER tagger which is much more robust than the nltk NER tagger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from nltk.tag import StanfordNERTagger\n",
    "#from nltk.tokenize import word_tokenize\n",
    "\n",
    "#st = StanfordNERTagger('C:\\\\Users\\\\Anubhav\\\\Desktop\\\\git\\\\iems308\\\\assignment4\\\\stanford\\\\stanford-ner-2018-02-27\\\\classifiers\\\\english.muc.7class.distsim.crf.ser',\n",
    "#                       'C:/Users/Anubhav/Desktop/git/iems308/assignment4/stanford/stanford-ner-2018-02-27/stanford-ner.jar',\n",
    "#                       encoding='utf-8')\n",
    "\n",
    "#text = 'While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.'\n",
    "\n",
    "#tokenized_text = word_tokenize(text)\n",
    "#classified_text = st.tag(tokenized_text)\n",
    "\n",
    "#print(classified_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
